(self.webpackChunkwebsite_querystar_io=self.webpackChunkwebsite_querystar_io||[]).push([[979],{5009:(e,t,n)=>{e.exports={src:{srcSet:n.p+"assets/ideal-img/fig_1.48d8f1a.640.png 640w,"+n.p+"assets/ideal-img/fig_1.981d6a3.1030.png 1030w",images:[{path:n.p+"assets/ideal-img/fig_1.48d8f1a.640.png",width:640,height:340},{path:n.p+"assets/ideal-img/fig_1.981d6a3.1030.png",width:1030,height:547}],src:n.p+"assets/ideal-img/fig_1.48d8f1a.640.png",toString:function(){return n.p+"assets/ideal-img/fig_1.48d8f1a.640.png"},placeholder:void 0,width:640,height:340},preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAwUlEQVQImT3Mv2rCQADH8XsBnQSXTqWLU6CDQwf7AAqd3Do5FYoP4NqhfYSOboq4dmifpJxSpPlDEqeEXC7HkdxXksHf9oMvH2HrmjRNkYcDUkqUUtRNw+nfRx7/+JVHojhBRFFAURRkWUae52hdkqRnRg9TbrxH+rf3LFdviErrLrDWdlr7gyimdzdGDEcIMeDp+QVRGUOp1DVs9bLUTLwZ89mC9XbD1/cPIgz9TmwjYwxaa5xzvL9+sP/c0c45ywXG7qUGJ1gw4wAAAABJRU5ErkJggg=="}},1751:(e,t,n)=>{e.exports={src:{srcSet:n.p+"assets/ideal-img/fig_2.771ced0.640.png 640w,"+n.p+"assets/ideal-img/fig_2.52a0fc7.1030.png 1030w",images:[{path:n.p+"assets/ideal-img/fig_2.771ced0.640.png",width:640,height:312},{path:n.p+"assets/ideal-img/fig_2.52a0fc7.1030.png",width:1030,height:503}],src:n.p+"assets/ideal-img/fig_2.771ced0.640.png",toString:function(){return n.p+"assets/ideal-img/fig_2.771ced0.640.png"},placeholder:void 0,width:640,height:312},preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAFCAYAAAB8ZH1oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAaklEQVQImU2OCw4AMQRE3f+0/WnR6mzYbLISIeMN6N6LCHeHmWGMgdYaRCS1iGAoGwDnHKhqwr337EP75sTMMFO4v2BsKqVk/cxrTdBkhopg7421VmaAc87URBXBUNyPDHcORFBrzRf+Pz6acsQb88BMfwAAAABJRU5ErkJggg=="}},2367:(e,t,n)=>{e.exports={src:{srcSet:n.p+"assets/ideal-img/fig_3.43b503c.640.png 640w,"+n.p+"assets/ideal-img/fig_3.5b13f0a.701.png 701w",images:[{path:n.p+"assets/ideal-img/fig_3.43b503c.640.png",width:640,height:450},{path:n.p+"assets/ideal-img/fig_3.5b13f0a.701.png",width:701,height:493}],src:n.p+"assets/ideal-img/fig_3.43b503c.640.png",toString:function(){return n.p+"assets/ideal-img/fig_3.43b503c.640.png"},placeholder:void 0,width:640,height:450},preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAHCAYAAAAxrNxjAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAqklEQVQYlYWOywrCMBBF88FuXAuCD7C7bnwU63eJGzWv4qZFk0hKmsiVBBpw5cDZXGbuHFLOSuyWe9TFKXPc1DisKhTTAovJGtt5BfJWFm5w+DekOVPcrzd0XQdrLYwx0FrDe48QAoIP+IQA4r2GFBycc0gpM0KIlFFKoZQCMU+bwqZp8lLbtj/0fQ/yuAhwxtJVfB1D51wmOw7uBc4ZGGWpOTaOGtF7dP8C9Qv+p+bgOv0AAAAASUVORK5CYII="}},4138:(e,t,n)=>{e.exports={src:{srcSet:n.p+"assets/ideal-img/fig_4.e1bc67e.640.png 640w,"+n.p+"assets/ideal-img/fig_4.49caf7b.878.png 878w",images:[{path:n.p+"assets/ideal-img/fig_4.e1bc67e.640.png",width:640,height:628},{path:n.p+"assets/ideal-img/fig_4.49caf7b.878.png",width:878,height:861}],src:n.p+"assets/ideal-img/fig_4.e1bc67e.640.png",toString:function(){return n.p+"assets/ideal-img/fig_4.e1bc67e.640.png"},placeholder:void 0,width:640,height:628},preSrc:"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAoAAAAKCAYAAACNMs+9AAAACXBIWXMAAAsTAAALEwEAmpwYAAABgElEQVQYlVWMT0iTARxAf0PE2WbTyZbWNjcHn3O6bOpcpR0MKkh2KEGjQkHwz0E8SJAXCUQ6i9jBPEStsSBTE/HSxYuHjqk0XFtIE73Y+sYQdN/ohenFB+/04MnrN2HudfcSejJEx+OBc4aeDnH7QQ9r61+RvpHnyAUHuks+pNyDmFyI0XGqyYVIGW8/LCD9o+OIyc3F6hb09msUXq6jsNJLQUUtels9YrATmf+MDD57gZhrMLkDlLj8GBwNGJyNFFc1Y1GaELOT6OLK2dHixVjdQom7Gb2tASlXEIuCzlqDFFXwai6MDI9N/A+lTj9G+1XMyg2s3ltU+a5zxdeGzuph8uUU8iOW4I6ng4fBRwQsrbyfjRJPxNnY3GR3b4/vsRjb8W1k/9c+95UQnf4u2m13+TK/SlpNsrPzjdTuFqr6k2z2AEklUwRKbxIsa0URL0vvlvlLnnT6N5nMH1Q1jablkMPsIR/nPhGZiRKejpCMJTkhd5wjn89zdHSMpmn8A7Al4b1LQ39FAAAAAElFTkSuQmCC"}},9234:(e,t,n)=>{"use strict";n.r(t),n.d(t,{contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>l,toc:()=>p});var a=n(7462),o=(n(7294),n(4137)),i=n(5944);const s={title:"Retrieval Augmented Generation (RAG) Enhanced Workflow Automations",description:"Retrieval Augmented Generation (RAG) Enhanced Workflow Automations",keywords:["Retrieval Augmented Generation","Slack Bot","LlamaIndex","Workflow Automation","Large Language Models","In-Context Learning","QueryStar"],image:"http://querystar.io/assets/images/fig_1-dde0f1512adc6eccd5fed2d952e79de6.png"},r="Retrieval Augmented Generation (RAG) Enhanced Workflow Automations",l={type:"mdx",permalink:"/rag/",source:"@site/src/pages/rag/index.mdx",title:"Retrieval Augmented Generation (RAG) Enhanced Workflow Automations",description:"Retrieval Augmented Generation (RAG) Enhanced Workflow Automations",frontMatter:{title:"Retrieval Augmented Generation (RAG) Enhanced Workflow Automations",description:"Retrieval Augmented Generation (RAG) Enhanced Workflow Automations",keywords:["Retrieval Augmented Generation","Slack Bot","LlamaIndex","Workflow Automation","Large Language Models","In-Context Learning","QueryStar"],image:"http://querystar.io/assets/images/fig_1-dde0f1512adc6eccd5fed2d952e79de6.png"}},p=[{value:"Putting RAG into perspective",id:"putting-rag-into-perspective",level:3},{value:"Modern vs. traditional workflow automations",id:"modern-vs-traditional-workflow-automations",level:3},{value:"Ingredients for an RAG based Workflow Automation",id:"ingredients-for-an-rag-based-workflow-automation",level:2},{value:"Trigger-Action Framework\xa0",id:"trigger-action-framework",level:3},{value:"RAG framework",id:"rag-framework",level:3},{value:"The LLM",id:"the-llm",level:3},{value:"The recipe",id:"the-recipe",level:2},{value:"Step 1: Build the\xa0RAG",id:"step-1-build-therag",level:3},{value:"Step 2: Build the\xa0Bot",id:"step-2-build-thebot",level:3},{value:"Step 3: Running the\xa0app.py",id:"step-3-running-theapppy",level:3},{value:"Conclusions",id:"conclusions",level:2}],d={toc:p},m="wrapper";function u(e){let{components:t,...s}=e;return(0,o.kt)(m,(0,a.Z)({},d,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"retrieval-augmented-generation-rag-enhanced-workflow-automations"},"Retrieval Augmented Generation (RAG) Enhanced Workflow Automations"),(0,o.kt)(i.Z,{img:n(5009),mdxType:"Image"}),(0,o.kt)("p",null,'Workflow automations have predominantly encompassed simple "if-this-then-that" (IFTTT) tasks that supported simple data transfer from one task to another. For example: when a Google form gets submitted, it\'s content is sent as a slack message. These automations are often referred to as Trigger-Action based workflows, where a trigger (such as a new email, slack message, a signup, a form submit) would set off a train of actions (such as parsing a message, adding a row to Google Sheets, sending a notification). Performing simple information transfer have been the crux of these workflows, and have mostly evolved horizontally to integrate with the vast array of SaaS tools ranging from Google Sheets/Docs/Drive, Mailchimp, Slack, Confluence, Jira, Notion etc.'),(0,o.kt)("p",null,"The latest advances in Large Language Models (LLM) show great potential to enhance our capabilities to build workflows that are beyond simple triggers and actions. As an example, there have been an increasing number of tools that showcase Slack bots performing specific business logic-related tasks, such answering questions based on data stored in data warehouses (sql generation & execution), answering questions based on documents (retrieval and summarisation), and so on."),(0,o.kt)("h3",{id:"putting-rag-into-perspective"},"Putting RAG into perspective"),(0,o.kt)("p",null,"One of the common tasks in automations, as mentioned above, have been to answer questions based on certain documents. Which also happens to be a widely recognized use-case for LLMs. However, LLMs being general completion models require domain specific context when it comes to answering domain specific questions. This is where In-Context Learning (ICL) implemented using Retrieval Augmented Generation (RAG) shines. Okay, that sounds like a mouthful, let me unpack it."),(0,o.kt)("p",null,"LLMs can be trained on, and/or fine tuned with a corpus of relevant documents to answer questions on those documents. However, the cost of model training and fine-tuning can be high. Another approach is to use In-Context Learning: adding relevant information about the question into a prompt, then using an LLM to generate an answer. This approach is geared towards prompt engineering, where the prompts must include all relevant information about the question."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"But, how do we get relevant information about the question?")),(0,o.kt)("p",null,'There are many algorithms being developed to do this task. Here, we will mainly discuss the "Embedding Search" approach. We first store a document as chunks of text along with their embedding vectors. This step can give us a simple Knowledge Base. Then, we use vector similarity to find chunks of text that are relevant to the question. This will be explained in detail in later sections. With the relevant information, a prompt can be finally designed to answer our question.'),(0,o.kt)("h3",{id:"modern-vs-traditional-workflow-automations"},"Modern vs. traditional workflow automations"),(0,o.kt)("p",null,"As can be seen from above, the workflow above has a complicated maze of data ingestion, processing tasks, and finally generation of the answer (result). Contrasting this with traditional workflows of simple if-this-then-that tasks begs us to consider a more programmatic approach where we can make use of state of the art libraries such as LangChain, Llama-index, and access custom APIs."),(0,o.kt)("p",null,"To understand better the new evolution of workflow automations, lets dive deep into an example of document based question-answering, by designing the triggers-actions, and implementing RAG. The use case would be to implement a Slack bot to answer a question posted to a channel. The question will be based on a few documents."),(0,o.kt)("h2",{id:"ingredients-for-an-rag-based-workflow-automation"},"Ingredients for an RAG based Workflow Automation"),(0,o.kt)(i.Z,{img:n(1751),mdxType:"Image"}),(0,o.kt)("p",null,"The three main ingredients, or perhaps the only ingredients you'd need are:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"A framework to listen to triggers, and execute actions or tasks."),(0,o.kt)("li",{parentName:"ol"},"A framework to implement Retrieval Augmented Generation."),(0,o.kt)("li",{parentName:"ol"},"An LLM to work with.")),(0,o.kt)("p",null,"Assuming that you'd be using Python, here are your options."),(0,o.kt)("h3",{id:"trigger-action-framework"},"Trigger-Action Framework\xa0"),(0,o.kt)("p",null,"Since the use case we are implementing is to listen to a Slack channel for a message, and reply an answer to that message in Slack there are two options:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"https://github.com/SlackAPI/bolt-python"},"bolt-python")," the official Python SDK from Slack to work with Slack APIs. Using this requires you to setup an App and connect it to your workspace you are an admin of, secure all the scopes needed for the task, setup an event listener endpoint or use a websocket connection provided with the SDK to listen to incoming messages that match your trigger conditions. As this setup is far beyond the scope of what we want to discuss in this post, we will opt for an easier alternative."),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"https://github.com/modelstar-labs/querystar"},"querystar")," is a Python framework that let's you setup integrations and work with popular SaaS tools like Slack and Google Sheets without the fuss of authentication, access control, setting up event listeners using web hooks.")),(0,o.kt)("p",null,"We will use ",(0,o.kt)("a",{parentName:"p",href:"https://querystar.io/"},"QueryStar")," for the use case we are implementing. Here is a ",(0,o.kt)("a",{parentName:"p",href:"https://querystar.io/docs/quickstart/token/"},"quickstart")," on how to get started with it."),(0,o.kt)("h3",{id:"rag-framework"},"RAG framework"),(0,o.kt)("p",null,"There are quite a few frameworks available to implement this, the most popular ones are:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("a",{parentName:"p",href:"https://www.langchain.com/"},"LangChain")," is a very popular framework in Python to work with LLMs,. It provides APIs to create embeddings, and build knowledge bases. That being said, LangChain is a general framework for prompt engineering to build prompt chains, observe and debug etc. RAG is only one of the many built-in modules.")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("a",{parentName:"p",href:"https://github.com/jerryjliu/llama_index"},"LlamaIndex")," on the other hand is primarily built as a data framework to work with LLMs. It has out of the box functions to create Knowledge Bases, and implement RAG, and connect to LLMS to implement question-answering workflows."))),(0,o.kt)("p",null,"We will use LlamaIndex for implementing our use case. You can find further documentation on ",(0,o.kt)("a",{parentName:"p",href:"https://gpt-index.readthedocs.io/en/latest/"},"LlamaIndex")," here."),(0,o.kt)("h3",{id:"the-llm"},"The LLM"),(0,o.kt)("p",null,"The final ingredient is the Large Language Model. There are plenty of LLM options available at this point to choose from. They differ in openness and utility for specific purposes."),(0,o.kt)("p",null,"We will use OpenAI's LLM in implementing our case. Get yourself a token from ",(0,o.kt)("a",{parentName:"p",href:"https://platform.openai.com/"},"here"),"."),(0,o.kt)("h2",{id:"the-recipe"},"The recipe"),(0,o.kt)("p",null,"With all the ingredients in place let's get started with the use case."),(0,o.kt)("h3",{id:"step-1-build-therag"},"Step 1: Build the\xa0RAG"),(0,o.kt)("p",null,"The documents we will be asking questions on, are the documentation of the LlamaIndex library. A sample question we could ask is:"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Question: In llama-index, what is a knowledge graph?")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("em",{parentName:"strong"},"Data Preparation"))),(0,o.kt)("p",null,"Let's start with downloading/cloning ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/jerryjliu/llama_index/tree/main/docs"},"LlamaIndex's doc folder")," to a local folder ",(0,o.kt)("inlineCode",{parentName:"p"},"ask_llamaindex_slack_bot"),"."),(0,o.kt)("p",null,"There're many types of files in the folder. Some files (like\xa0",(0,o.kt)("inlineCode",{parentName:"p"},".py")," or\xa0",(0,o.kt)("inlineCode",{parentName:"p"},".bat"),") do not contain too much context. So, we only want to keep\xa0",(0,o.kt)("inlineCode",{parentName:"p"},".md")," and\xa0",(0,o.kt)("inlineCode",{parentName:"p"},".rst")," files:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"# Data cleaning\ndir_path = './lidocs'  #local folder of LlamaIndex documentations\nfor root, dirs, files in os.walk(dir_path):\n    for file in files:\n        if file.endswith(('.md', '.rst')):\n            continue\n        else:\n            file_path = os.path.join(root, file)\n            os.remove(file_path)\n")),(0,o.kt)("p",null,"Once the doc folder is clean, it's very convenient to use ",(0,o.kt)("inlineCode",{parentName:"p"},"SimpleDirectoryReader")," function provided in ",(0,o.kt)("inlineCode",{parentName:"p"},"llama_index")," to load the entire folder to ",(0,o.kt)("inlineCode",{parentName:"p"},"lidocs")," object at once."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'from llama_index import SimpleDirectoryReader\n\nreader = SimpleDirectoryReader(input_dir="./lidocs", recursive=True)\nlidocs = reader.load_data()\n')),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("em",{parentName:"strong"},"Document Indexing\u200b"))),(0,o.kt)("p",null,"Now we're ready to build the ",(0,o.kt)("inlineCode",{parentName:"p"},"index"),": dividing each document into chunks and embedding them. ",(0,o.kt)("inlineCode",{parentName:"p"},"LlamaIndex")," has a great API for this: ",(0,o.kt)("inlineCode",{parentName:"p"},"VectorStoreIndex.from_documents()"),". Then we store ",(0,o.kt)("inlineCode",{parentName:"p"},"index")," in files."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"import openai\nfrom llama_index import VectorStoreIndex\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\nindex = VectorStoreIndex.from_documents(lidocs)\n# save index to files\nindex.storage_context.persist()\n")),(0,o.kt)("p",null,"The storage folder automatically appears:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"\u251c\u2500\u2500 ask_llamaindex_slack_bot\n\u2502 \u251c\u2500\u2500 storage\n\u2502 \u2502 \u251c\u2500\u2500 docstore.json\n\u2502 \u2502 \u251c\u2500\u2500 graph_store.json\n\u2502 \u2502 \u251c\u2500\u2500 index_store.json\n\u2502 \u2502 \u2514\u2500\u2500 vector_store.json\n")),(0,o.kt)("p",null,"All the ",(0,o.kt)("inlineCode",{parentName:"p"},"embedding")," vectors are saved in ",(0,o.kt)("inlineCode",{parentName:"p"},"vector_store.json"),". The file size is 33 MB, which contains a mathematical representation of the entire LlamaIndex's documentation."),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"In this step, we use GPT, a commercial model service by OpenAI."),(0,o.kt)("p",{parentName:"admonition"},"Before building the index, make sure you have an OpenAI API key. This step may cost you up to $1 for the tokens used. To avoid the cost, you can skip this step and download ",(0,o.kt)("em",{parentName:"p"},(0,o.kt)("a",{parentName:"em",href:"https://github.com/modelstar-labs/querystar-demo/tree/main/ask_llamaindex_slack_bot/storage"},"the ",(0,o.kt)("inlineCode",{parentName:"a"},"storage")," folder from QueryStar demo repo")),"."),(0,o.kt)("p",{parentName:"admonition"},(0,o.kt)("inlineCode",{parentName:"p"},"VectorStoreIndex.from_documents()")," call may take 2-5 mins to finish, highly depending on API latency in your region.\u200b")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("em",{parentName:"strong"},"RAG Function"))),(0,o.kt)("p",null,"With the index file in place, we can finally build the ",(0,o.kt)("inlineCode",{parentName:"p"},"RAG")," function. Again, it's simple with ",(0,o.kt)("inlineCode",{parentName:"p"},"LlamaIndex"),". We just need to load them to the ",(0,o.kt)("inlineCode",{parentName:"p"},"index")," object, and use the built-in query engine to get ",(0,o.kt)("inlineCode",{parentName:"p"},"response"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"from llama_index import StorageContext, load_index_from_storage\nimport os, openai\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ndef ask_llamaindex(question: str):\n    # rebuild storage context\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n    # load index\n    index = load_index_from_storage(storage_context)\n    query_engine = index.as_query_engine()\n    response = query_engine.query(question)\n\n    return response\n")),(0,o.kt)("p",null,"This will the function we will use to answer questions."),(0,o.kt)("h3",{id:"step-2-build-thebot"},"Step 2: Build the\xa0Bot"),(0,o.kt)("p",null,"Before running any code in this section, please make sure you've already ",(0,o.kt)("a",{parentName:"p",href:"https://querystar.io/docs/quickstart/token/"},"gotten a QueryStar token"),", ",(0,o.kt)("a",{parentName:"p",href:"https://querystar.io/docs/quickstart/install/"},"installed the library"),", and ",(0,o.kt)("a",{parentName:"p",href:"https://querystar.io/docs/quickstart/coding/"},"can run the ",(0,o.kt)("inlineCode",{parentName:"a"},"hello world")," Slackbot"),". The setup process should only take you less than 10 mins."),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"QueryStar helps integrate easily with 3rd party API services which also include Slack authorization. So, we do NOT need a Slack token here.")),(0,o.kt)("p",null,"So, the workflow to implement a question-answering bot in Slack are:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Listen for a new message event in a channel. This means the app would be notified whenever a new message, in our case a question is posted to a Slack channel."),(0,o.kt)("li",{parentName:"ol"},"Get the answer for the question, using RAG implemented in the section above. We will use the function ",(0,o.kt)("inlineCode",{parentName:"li"},"ask_llamaindex()")," we created above."),(0,o.kt)("li",{parentName:"ol"},"Send the answer as a reply to the question in Slack.")),(0,o.kt)("p",null,"Since (2) is already implemented above, let's see how to implement (1) and (3)."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("em",{parentName:"strong"},"Listen to new messages in Slack"))),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"querystar")," provides a convenient function to receive notifications of new messages posted to a channel in Slack. These functions, which you use to listen to events are named triggers in ",(0,o.kt)("inlineCode",{parentName:"p"},"querystar"),". For a Slack message we would use the ",(0,o.kt)("inlineCode",{parentName:"p"},"querystar.triggers.slack.new_message()")," function. Here is how you can use it:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"import querystar as qs\n\ndata = qs.triggers.slack.new_message(channel_id='C05PRRJ0H4N',\n                                     trigger_string='ask llama')\n")),(0,o.kt)("p",null,"This script is set to listen to a new Slack message. When a message matches the filter condition (",(0,o.kt)("inlineCode",{parentName:"p"},"channel_id")," AND ",(0,o.kt)("inlineCode",{parentName:"p"},"trigger_string"),"), a ",(0,o.kt)("inlineCode",{parentName:"p"},"dict")," object of the message will return to variable ",(0,o.kt)("inlineCode",{parentName:"p"},"data"),"."),(0,o.kt)("p",null,"Here is an example of the return:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'{\n    "client_msg_id":"...",\n    "type":"message",\n    "text":"ask llama: ...",\n    "user":"useid...",\n    "ts":"1693582549.746649",\n    "blocks":[...],\n    "team":"...",\n    "thread_ts":"1693536314.270179",\n    "parent_user_id":"...",\n    "channel":"C05PRRJ0H4N",\n    "event_ts":"1693582549.746649",\n    "channel_type":"group"\n}\n')),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("em",{parentName:"strong"},"Send reply to the question in Slack\u200b"))),(0,o.kt)("p",null,"From the message data we received above, we can extract the question from the ",(0,o.kt)("inlineCode",{parentName:"p"},"text")," property of the data ",(0,o.kt)("inlineCode",{parentName:"p"},"dict"),". And using the ",(0,o.kt)("inlineCode",{parentName:"p"},"ask_llamaindex")," function above we can get an answer:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"question = data.get('text', None)\nanswer = ask_llamaindex(question)\n")),(0,o.kt)("p",null,"Then, we use ",(0,o.kt)("inlineCode",{parentName:"p"},"actions.slack.add_message()")," to post the answer back to the same channel."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"from querystar import qs\n\n# send answer to Slack\nqs.actions.slack.add_message(channel_id='C05PRRJ0H4N',\n                              message=f'Answer is: {answer}')\n")),(0,o.kt)("p",null,"Let's put all of these pieces together into a complete workflow in app.py\xa0:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"import os\nimport openai\nfrom llama_index import StorageContext, load_index_from_storage\nimport querystar as qs\n\nopenai.api_key = os.getenv('OPENAI_API_KEY')\n\ndef ask_llamaindex(question: str):\n    # rebuild storage context\n    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n    # load index\n    index = load_index_from_storage(storage_context)\n    query_engine = index.as_query_engine()\n    response = query_engine.query(question)\n\n    return response\n\n# Listen for a question in Slack channel\ndata = qs.triggers.slack.new_message(channel_id='C05PRRJ0H4N',\n                                     trigger_string='ask llama')\n\n# Find the answer\nquestion = data.get('text', None)\nanswer = ask_llamaindex(question)\n\n# Send answer to back to Slack channel\nqs.actions.slack.add_message(channel_id='C05PRRJ0H4N',\n                              message=f'Answer is: {answer}')\n")),(0,o.kt)("h3",{id:"step-3-running-theapppy"},"Step 3: Running the\xa0app.py"),(0,o.kt)("p",null,"Open your terminal, run this command inside the folder ",(0,o.kt)("inlineCode",{parentName:"p"},"ask_llamaindex_slack_bot")," with your Python environment activated that contains the requirements of ",(0,o.kt)("inlineCode",{parentName:"p"},"querystar"),", ",(0,o.kt)("inlineCode",{parentName:"p"},"openai"),", and ",(0,o.kt)("inlineCode",{parentName:"p"},"llama-index")," installed."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-shell"},"querystar run app.py\n")),(0,o.kt)("p",null,"Asking a question in the specified Slack channel would result in:"),(0,o.kt)(i.Z,{img:n(2367),mdxType:"Image"}),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("em",{parentName:"strong"},"Now, you can invite your entire team to join this channel and learn LlamaIndex together!"))),(0,o.kt)("h2",{id:"conclusions"},"Conclusions"),(0,o.kt)("p",null,"Tasks that were once limited to specific trained language models are now very easy to integrate and use in workflows thanks to the development of Large Language Models. This has opened up the possibilities to explore more powerful workflow automations. With a programmatic approach you can use many state of the art libraries: for example Llama-Index and LangChain, or even in house libraries. A quick and easy approach to automate these workflows in Python is using ",(0,o.kt)("a",{parentName:"p",href:"https://querystar.io/"},"QueryStar"),". In this article we've seen an end-to-end implementation of a Slack bot, which represents a use case of workflow automation, to answer questions based on a certain set of documents."),(0,o.kt)(i.Z,{img:n(4138),mdxType:"Image"}),(0,o.kt)("p",null,"Links to the frameworks used in this article:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"QueryStar: ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/modelstar-labs/querystar"},"https://github.com/modelstar-labs/querystar")),(0,o.kt)("li",{parentName:"ul"},"LlamaIndex: ",(0,o.kt)("a",{parentName:"li",href:"https://github.com/jerryjliu/llama_index"},"https://github.com/jerryjliu/llama_index"))))}u.isMDXComponent=!0}}]);